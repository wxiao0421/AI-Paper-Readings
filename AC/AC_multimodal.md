# AC - Multi-modal Data
|Paper|Conference|Remarks
|--|--|--|
|[A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions](https://ibug.doc.ic.ac.uk/media/uploads/documents/PAMI-AVemotionSurvey-CAMERA.pdf)|IEEE TPAMI 2009|1. Discuss human emotion perception from a psychological perspective. 2. Examine available approaches for emotion recognition, and important issues like the collection and availability of training and test data. 3. Outline some challenges for affect recognition|
|[Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications](https://ieeexplore.ieee.org/document/5520655/)|IEEE TAC 2010|1.  Discusses theoretical perspectives that view emotions as expressions, embodiments, outcomes of cognitive appraisal, social constructs, products of neural circuitry, and psychological interpretations of basic feelings.  2. Provides meta-analyses on existing reviews of affect detection systems that focus on traditional affect detection modalities like physiology, face, and voice, and also reviews emerging research on more novel channels such as text, body language, and complex multi-modal systems|
|[Multimodal Affect Recognition using Kinect](https://arxiv.org/pdf/1607.02652)|Arxiv 2016|This research used color and depth sensing devices such as Kinect for facial feature extraction and tracking human body joints. Features: temporal features across multiple frames; decision level fusion with majority voting; combination of emotion templates and supervised learning. Results: better recognition than supervised learning alone; better recognition than position based features only.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTU4MTI2OTExLDE5OTE5NTkyNDYsLTEyNz
EyMTM1MDFdfQ==
-->