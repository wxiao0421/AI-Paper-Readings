# ML - Transformer
|Paper|Conference|Remarks
|--|--|--|
|[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)|NIPS 2017|1. Propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 2. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.|
|[# Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247)|PBML 2018|1. Describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer. 2. Examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers.|
[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkwMzE1MTI5Ml19
-->