# NLP - Dialogue System
|Paper|Conference|Remarks
|--|--|--|
|[POMDP-based Statistical Spoken Dialogue Systems: a Review](http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf)|Proceedings of IEEE 2013|1. By including an explicit Bayesian model of uncertainty and by optimizing the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide a data-driven framework for dialog modelling. 2. Provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.|
|[A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](https://arxiv.org/pdf/1506.06714)|NAACL 2015|Integrate context information into dialogue generation|
|[A Neural Conversational Model](https://arxiv.org/pdf/1506.05869)|Arxiv 2015|1. Present a simple approach for conversation modelling which uses the recently proposed sequence to sequence framework. 2. It can be trained end-to-end and thus requires much fewer hand-crafted rules|
|[A Persona-Based Neural Conversation Model](http://www.aclweb.org/anthology/P16-1094)|ACL 2016|1. Present persona-based models for handling the issue of speaker consistency in neural response generation. 2. Yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.|
|[Deep Reinforcement Learning for Dialogue Generation](https://aclweb.org/anthology/D16-1127)|EMNLP 2016|1. Current conversational agents tend to be short-sighted and ignoring long-term interactivity. 2. Combination of traditional NLP model with reinforcement learning to model future rewards in chatbot dialogue to learn a neural conversational model based on long-term success of dialogues. 3. Rewards: informative, coherence and ease-of-answering. 4. Model evaluation metrices: diversity, conversation length and human judges|
|[How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation](https://aclweb.org/anthology/D16-1230)|EMNLP 2016|1. Investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. 2. Show that current automatic metrics correlate very weakly with human judgement in the non-technical Twitter domain, and not at all in the technical Ubuntu domain|
|[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)|AAAI 2016|1. Extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. 2. Investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings|
|[Topic Aware Neural Response Generation](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14563/14260)|AAAI 2016|1. Consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. 2. Utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability.|
|[A Context-aware Natural Language Generator for Dialogue Systems](https://arxiv.org/pdf/1608.07076)|SIGDIAL 2016|Present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses.|
|[A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/pdf/1611.08562)|Arxiv 2016|1. Propose a simple, fast decoding algorithm that fosters diversity in neural generation. 2. Diverse decoding helps across tasks of dialogue response generation, abstractive summarization and machine translation, especially those for which reranking is needed|
|[A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues](https://arxiv.org/pdf/1605.06069)|Arxiv 2016|1. Propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps to model complex dependencies between subsequences.|
|[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424)|Arxiv 2016|1. Propose DBS to decode a list of diverse outputs by optimizing for a diversity-augmented objective. 2. Observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space|
|[Generative Deep Neural Networks for Dialogue: A Short Review](https://arxiv.org/pdf/1611.06216)|Arxiv 2016|Review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.|
|[On the Evaluation of Dialogue Systems with Next Utterance Classification](https://arxiv.org/pdf/1605.05414)|Arxiv 2016|1. Investigate the performance of humans on this task to validate the relevance of Next Utterance Classification (NUC) as a method of evaluation. 2. Results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.|
|[Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders](https://arxiv.org/pdf/1703.10960)|ACL 2017| 1. Present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. The proposed model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. 2. Further develop a novel variant that is integrated with linguistic prior knowledge for better performance. 3. Propose a bag-of-word loss to alleviate the problem of vanishing KL loss.|
|[Learning End-to-End Goal-Oriented Dialog](https://arxiv.org/pdf/1605.07683)|ICLR 2017| 1. Proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. 2. Show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.|
|[Adversarial Learning for Neural Dialogue Generation](https://www.aclweb.org/anthology/D17-1230)|EMNLP 2017| 1. Propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. 2. Cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. 3. Some appealing research directions.|
|[Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models](https://arxiv.org/pdf/1701.03185)|EMNLP 2017| 1. Add self-attention to the decoder to maintain coherence in longer responses, and propose a practical approach, called the glimpse-model, for scaling to large datasets. 2. Introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process.|
|[ParlAI: A Dialog Research Software Platform](http://aclweb.org/anthology/D17-2014)|EMNLP 2017| A unified open-source framework in Python for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others' models, and improving upon existing architectures.|
|[A Network-based End-to-End Trainable Task-oriented Dialogue System](https://arxiv.org/pdf/1604.04562)|EACL 2017| 1. Introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. 2. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand.|
|[Building Task-Oriented Dialogue Systems for Online Shopping](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14261/13975)|AAAI 2017| 1. Present a general solution towards building task-oriented dialogue systems for online shopping, aiming to assist online customers in completing various purchase-related tasks, such as searching products and answering questions, in a natural language conversation manner. 2. Show what & how existing NLP techniques, data resources, and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage.|
|[End-to-End Task-Completion Neural Dialogue Systems](https://arxiv.org/pdf/1703.01008)|IJCNLP 2017| 1. Presents a novel end-to-end learning framework for task-completion dialogue systems. 2. The proposed system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. 3. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system.|
|[A Survey on Dialogue Systems: Recent Advances and New Frontiers](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Poetry Generation](https://pdfs.semanticscholar.org/d89d/053b1c2481088b1af2bd36e0a6d959ff1373.pdf)|SPECOM 2017| 1. Describe a new system for the automatic creation of poetry in Basque that not only generates novel poems, but also creates them conveying a certain attitude or state of mind. 2. The proposed system receives as an input the topic of the poem and the affective state (positive, neutral or negative) and tries to give as output a novel poem that: (1) satisfies formal constraints of rhyme and metric, (2) shows coherent content related to the given topic, and (3) expresses them through the predetermined mood.|
|[Emotional Human-Machine Conversation Generation Based on Long Short-Term Memory](https://link.springer.com/article/10.1007/s12559-017-9539-4)|Cognitive Computation 2017| 1. Propose a new model based on long short-term memory, which is used to achieve an encoder-decoder framework, and we address the emotional factor of conversation generation by changing the model’s input using a series of input transformations: a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.|
|[A Deep Reinforcement Learning Chatbot](https://arxiv.org/pdf/1709.02349)|Arxiv 2017| 1. Consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. 2. The system has been trained to select an appropriate response from the models in its ensemble.|
|[Building Chatbot with Emotions](http://web.stanford.edu/class/cs224s/reports/Honghao_Wei.pdf)|N.A. 2017| 1. Aims at generating dialogues not only appropriate at content level, but also containing specific emotions. 2. Apply sentimental analysis on the dataset and pick up dialogue with strong emotion. 3. Apply deep reinforcement learning and introduce sentiment rewards during learning phase|
|[Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base](https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf)|NIPS 2018|1. Present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. 2. Introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances.|
|[Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network](http://www.aclweb.org/anthology/P18-1103)|ACL 2018|1. Investigate matching a response with its multi-turn context using dependency information based entirely on attention. 2. First, construct representations of text segments at different granularities solely with stacked self-attention. Second, extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network.|
|[Knowledge Diffusion for Neural Dialogue Generation](http://www.aclweb.org/anthology/P18-1138)|ACL 2018|1. Propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. 2. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities.|
|[Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures](http://aclweb.org/anthology/P18-1133)|ACL 2018|1. Propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. 2. Design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. 3. Propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude.|
|[Sentiment Adaptive End-to-End Dialog Systems](https://arxiv.org/pdf/1804.10731)|ACL 2018|1. Propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end dialog learning framework to make systems more user-adaptive and effective. 2. Incorporated user sentiment information in both supervised and reinforcement learning settings and gained improvements on a bus information search task.|
|[MojiTalk: Generating Emotional Responses at Scale](https://arxiv.org/pdf/1711.04090)|ACL 2018|1. Collect a large corpus of Twitter conversations that include emojis in the response, and assume the emojis convey the underlying emotions of the sentence. 2. Introduce a reinforced conditional variational encoder approach to train a deep generative model on these conversations, which allows us to use emojis to control the emotion of the generated text.|
|[Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems](https://arxiv.org/pdf/1804.08217)|ACL 2018|Propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to incorporate knowledge bases. |
|[Global-Locally Self-Attentive Dialogue State Tracker](https://arxiv.org/pdf/1805.09655)|ACL 2018|1. Propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. 2. The model uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. 3. Show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks.|
|[Variational Autoregressive Decoder for Neural Response Generation](http://aclweb.org/anthology/D18-1354)|EMNLP 2018|1. To solve the generation problem in VAE, this work proposes a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. 2. The approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation.|
|[Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](http://www.aclweb.org/anthology/N18-1193)|NAACL 2018|Propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history, in dyadic dialogue videos|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018|1. To address the KL-vanishing problem and inconsistent training objective in VAE, they propose to separate the training step into two phases: The first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Augmenting End-to-End Dialogue Systems with Commonsense Knowledge](https://arxiv.org/pdf/1709.05453)|AAAI 2018| 1. Investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. 2. Propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response|
|[A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/pdf/1702.01932)|AAAI 2018| 1. Presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. 2. Generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting.|
|[Neural Response Generation with Dynamic Vocabularies](https://arxiv.org/pdf/1711.11191)|AAAI 2018| 1. Propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. 2. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. 3. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary.|
|[Improving Variational Encoder-Decoders in Dialogue Generation](https://arxiv.org/pdf/1802.02032)|AAAI 2018| In VAE, encoder and decoder training are inconsistent, this work separate two phases: the first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding.|
|[A Hierarchical Latent Structure for Variational Conversation Modeling](https://arxiv.org/pdf/1804.03424)|NAACL 2018| 1. VAE for dialogue generation suffers from the generation problems: (1) the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables; (2) the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. 2. To solve the generation problem, this paper proposes a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization.|
|[Dialog Generation Using Multi-Turn Reasoning Neural Networks](https://arxiv.org/pdf/1804.03424)|NAACL 2018| Propose a generalizable dialog generation approach that adapts multiturn reasoning, one recent advancement in the field of document comprehension, to generate responses (“answers”) by taking current conversation session context as a “document” and current query as a “question”.|
|[Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge](https://arxiv.org/pdf/1809.03194)|CoNLL 2018| Proposes a novel neural network architecture for response selection in an end-to-end multi-turn conversational dialogue setting. The architecture applies context level attention and incorporates additional external knowledge provided by descriptions of domain-specific words.|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|
|[Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models](https://arxiv.org/pdf/1708.03152)|LREC 2018| 1. Propose speaker classification as a surrogate task for general speaker modeling, and collect massive data to facilitate research in this direction.|
|[Memory-augmented Dialogue Management for Task-oriented Dialogue Systems](https://arxiv.org/pdf/1805.00150)|Arxiv 2018| 1. Propose a novel Memory-Augmented Dialogue management model (MAD) which employs a memory controller and two additional memory structures, i.e., a slot-value memory and an external memory. 2.The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (for instance, cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks through storing more context information. 3. Propose slot-level attention on user utterances to extract specific semantic information for each slot to update the dialogue state efficiently|
|[WIZARD OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS](https://openreview.net/pdf?id=r1l73iRqKm)|ICLR 2019| 1. Collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. 2. Design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses.|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQwODMxODQ2MCwxOTkwODQ1MTYxLC0yMD
UyMDk4NDMsLTE1NTU3Mjg0LDE5NTA3Mzk1MjUsLTEzNDg2MjQ5
MjIsLTEwODA1NzYyNTQsLTU4MzE5Mzg1OCwtMTg0Mzg0NjE3OS
wxMTcwNDAxODA3LDUxMDI3ODY5NiwtOTU2NzkxMTc0XX0=
-->